# -*- coding: utf-8 -*-
"""Life bear

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V4spyKz9muG5pUZQxudQ_X9xJ7BLhmZw

**Import Pandas**
"""

import pandas as pd

"""**Read CSV, Display Head(10), Info & Describe**"""

# prompt: generate code to read csv "/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv" print  head(first 10) , info, description and change the separator  to a ';', set to low memory = 'True'

import pandas as pd

df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)

print(df.head(10))
print(df.info())
print(df.describe())

"""**Update Function To Clean Tail**"""

# prompt: update code to also print tail (10)

import pandas as pd


df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)

print(df.head(10))
print(df.tail(10))
print(df.info())
print(df.describe())

"""**Drop Duplicates in mail_address & login_id**"""

# prompt: update code to remove duplicates in mail_address and login_id

import pandas as pd


df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)

# Remove duplicates based on 'mail_address' and 'login_id'
df.drop_duplicates(subset=['mail_address', 'login_id'], keep='first', inplace=True)

print(df.head(10))
print(df.tail(10))
print(df.info())
print(df.describe())

"""**Separate CSV Into Chunks**"""

# prompt: Update code and Create 7 chunks using "/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv" print and export each chunk

import pandas as pd

df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)

# Remove duplicates based on 'mail_address' and 'login_id'
df.drop_duplicates(subset=['mail_address', 'login_id'], keep='first', inplace=True)

chunk_size = len(df) // 7

for i in range(7):
  start_index = i * chunk_size
  end_index = (i + 1) * chunk_size if i < 6 else len(df)
  chunk = df[start_index:end_index]

  print(f"Chunk {i+1}:")
  print(chunk.head(5))  # Print the first 5 rows of the chunk
  print("...")
  print(chunk.tail(5))  # Print the last 5 rows of the chunk
  print("-" * 20)

  # Export the chunk to a CSV file (optional)
  chunk.to_csv(f'/content/chunk_{i+1}.csv', index=False)

"""**Drop Emails with Invalid formats, Create a Directory For Chunks**"""

# prompt: update code and remove all emails with invalid formats and place them in a csv called dump

import pandas as pd
import os
import re

df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)

# Remove duplicates based on 'mail_address' and 'login_id'
df.drop_duplicates(subset=['mail_address', 'login_id'], keep='first', inplace=True)

# Regular expression pattern for valid email format
email_regex = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"

# Create a new DataFrame to store invalid emails
invalid_emails_df = pd.DataFrame(columns=df.columns)

# Iterate through the DataFrame and filter invalid emails
for index, row in df.iterrows():
  email = row['mail_address']
  if not re.match(email_regex, str(email)):
    invalid_emails_df = pd.concat([invalid_emails_df, pd.DataFrame([row])], ignore_index=True)
    df.drop(index, inplace=True)

# Create the "Chunks" directory if it doesn't exist
os.makedirs("/content/Chunks", exist_ok=True)

chunk_size = len(df) // 7

for i in range(7):
  start_index = i * chunk_size
  end_index = (i + 1) * chunk_size if i < 6 else len(df)
  chunk = df[start_index:end_index]

  print(f"Chunk {i+1}:")
  print(chunk.head(5))  # Print the first 5 rows of the chunk
  print("...")
  print(chunk.tail(5))  # Print the last 5 rows of the chunk
  print("-" * 20)

  # Export the chunk to a CSV file in the "Chunks" directory
  chunk.to_csv(f'/content/Chunks/chunk_{i+1}.csv', index=False)

# Export invalid emails to a CSV file named "dump.csv"
invalid_emails_df.to_csv('/content/dump.csv', index=False)

"""**Create a For Loop That Runs Code on all Chunks**"""

# prompt: update code to create a for loop using "/content/Chunks"

import pandas as pd
import os

df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)

# Remove duplicates based on 'mail_address' and 'login_id'
df.drop_duplicates(subset=['mail_address', 'login_id'], keep='first', inplace=True)

chunk_size = len(df) // 7

# Create the "Chunks" directory if it doesn't exist
os.makedirs("/content/Chunks", exist_ok=True)

for i in range(7):
  start_index = i * chunk_size
  end_index = (i + 1) * chunk_size if i < 6 else len(df)
  chunk = df[start_index:end_index]

  print(f"Chunk {i+1}:")
  print(chunk.head(5))  # Print the first 5 rows of the chunk
  print("...")
  print(chunk.tail(5))  # Print the last 5 rows of the chunk
  print("-" * 20)

  # Export the chunk to a CSV file in the "Chunks" directory
  chunk.to_csv(f'/content/Chunks/chunk_{i+1}.csv', index=False)

"""**Change "Created_at" Date Format**"""

# prompt: update code and change the date format in "created_at" to Day, Month, year and time

import pandas as pd
import os
import re
from datetime import datetime

df = pd.read_csv('/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv', sep=';', low_memory=True)

# Remove duplicates based on 'mail_address' and 'login_id'
df.drop_duplicates(subset=['mail_address', 'login_id'], keep='first', inplace=True)

# Regular expression pattern for valid email format
email_regex = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"

# Create a new DataFrame to store invalid emails
invalid_emails_df = pd.DataFrame(columns=df.columns)

# Iterate through the DataFrame and filter invalid emails
for index, row in df.iterrows():
  email = row['mail_address']
  if not re.match(email_regex, str(email)):
    invalid_emails_df = pd.concat([invalid_emails_df, pd.DataFrame([row])], ignore_index=True)
    df.drop(index, inplace=True)


# Convert 'created_at' column to datetime objects
df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')

# Format the datetime objects to "Day, Month, year and time"
df['created_at'] = df['created_at'].dt.strftime('%A, %B %d, %Y %I:%M %p')


# Create the "Chunks" directory if it doesn't exist
os.makedirs("/content/Chunks", exist_ok=True)

chunk_size = len(df) // 7

for i in range(7):
  start_index = i * chunk_size
  end_index = (i + 1) * chunk_size if i < 6 else len(df)
  chunk = df[start_index:end_index]

  print(f"Chunk {i+1}:")
  print(chunk.head(5))  # Print the first 5 rows of the chunk
  print("...")
  print(chunk.tail(5))  # Print the last 5 rows of the chunk
  print("-" * 20)

  # Export the chunk to a CSV file in the "Chunks" directory
  chunk.to_csv(f'/content/Chunks/chunk_{i+1}.csv', index=False)

# Export invalid emails to a CSV file named "dump.csv"
invalid_emails_df.to_csv('/content/dump.csv', index=False)

"""**Merge All Chunks**"""

# prompt: update code and merge chunk in file path "/content/Chunks" and export to a new csv called valid data

import pandas as pd
import os
import glob

# Create an empty list to store the data from each chunk
chunks_data = []

# Iterate through each CSV file in the "Chunks" directory
for filename in glob.glob('/content/Chunks/*.csv'):
  chunk_df = pd.read_csv(filename)
  chunks_data.append(chunk_df)

# Concatenate all chunks into a single DataFrame
merged_df = pd.concat(chunks_data, ignore_index=True)

# Export the merged DataFrame to a new CSV file named "valid_data.csv"
merged_df.to_csv('/content/valid_data.csv', index=False)

print("Merged data exported to 'valid_data.csv'")